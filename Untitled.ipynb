{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cee39b48-0d04-4bba-91f1-ed2c81486bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "df = pd.read_csv(\"ec2_pricing.csv\")\n",
    "raw_pricing_data = df[['API Name','Name', 'Compute Family','Instance Memory','vCPUs','On Demand']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3f2e332a-3298-42f3-9d5a-b918583b7c4e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "qa_pairs = []\n",
    "\n",
    "for idx, row in raw_pricing_data.iterrows():\n",
    "    api_name = row['API Name']\n",
    "    name = row['Name']\n",
    "    family = row['Compute Family']\n",
    "    memory = row['Instance Memory']\n",
    "    vcpus = row['vCPUs']\n",
    "    price = row['On Demand']\n",
    "    \n",
    "       # Convert to string if not already\n",
    "    memory = str(memory)\n",
    "    vcpus = str(vcpus)\n",
    "    price = str(price)\n",
    "    \n",
    "    questions = [\n",
    "        (f\"What is the API name for {name}?\", \n",
    "         f\"The API name for {name} is {api_name}. This instance is part of the {family} family, equipped with {memory} of RAM and {vcpus}, with an on-demand pricing of {price}.\"),\n",
    "        \n",
    "        (f\"What is the full name of {api_name}?\", \n",
    "         f\"The full name of {api_name} is {name}. It belongs to the {family} compute family and offers {memory} memory with {vcpus} processing power at {price}.\"),\n",
    "        \n",
    "        (f\"What compute family does {api_name} belong to?\", \n",
    "         f\"The {api_name} instance belongs to the {family} compute family. This instance type, also known as {name}, provides {memory} and {vcpus}, available at {price}.\"),\n",
    "        \n",
    "        (f\"How much memory does {api_name} have?\", \n",
    "         f\"The {api_name} instance has {memory} of memory. This {name} instance is a {family} type with {vcpus} and costs {price} on-demand.\"),\n",
    "        \n",
    "        (f\"How many vCPUs does {api_name} have?\", \n",
    "         f\"The {api_name} has {vcpus}. This {family} instance, named {name}, comes with {memory} of memory and is priced at {price}.\"),\n",
    "        \n",
    "        (f\"What is the on-demand hourly price for {api_name}?\", \n",
    "         f\"The on-demand hourly price for {api_name} is {price}. This {name} instance offers {memory} and {vcpus}, making it suitable for {family} workloads.\"),\n",
    "        \n",
    "        (f\"What is the hourly cost of {name}?\", \n",
    "         f\"The hourly cost of {name} ({api_name}) is {price}. This {family} instance provides {memory} of RAM and {vcpus} for your computing needs.\"),\n",
    "        \n",
    "        (f\"Which compute family is {name} part of?\", \n",
    "         f\"The {name} is part of the {family} compute family. Known by the API name {api_name}, it features {memory}, {vcpus}, and costs {price}.\"),\n",
    "        \n",
    "        (f\"What are the memory specifications for {name}?\", \n",
    "         f\"The {name} ({api_name}) has memory specifications of {memory}. This {family} instance also includes {vcpus} and is available at {price} on-demand.\"),\n",
    "        \n",
    "        (f\"How many virtual CPUs does {name} have?\", \n",
    "         f\"The {name} has {vcpus}. This instance, with API name {api_name}, is a {family} type offering {memory} and priced at {price}.\"),\n",
    "        \n",
    "        (f\"What is the price per hour for {api_name}?\", \n",
    "         f\"The price per hour for {api_name} is {price}. This {name} instance from the {family} family comes with {memory} and {vcpus}.\"),\n",
    "        \n",
    "        (f\"Tell me about the {api_name} instance specifications.\", \n",
    "         f\"The {api_name}, also known as {name}, is a {family} instance with {memory} of memory and {vcpus}. It's available on-demand at {price}.\"),\n",
    "        \n",
    "        (f\"What instance type has {memory} and {vcpus}?\", \n",
    "         f\"The instance type with {memory} and {vcpus} is {api_name} ({name}). This {family} instance is priced at {price} on-demand.\"),\n",
    "        \n",
    "        (f\"Compare the specs of {api_name}.\", \n",
    "         f\"The {api_name} ({name}) specifications include: Compute Family - {family}, Memory - {memory}, vCPUs - {vcpus}, and On-Demand Price - {price}.\"),\n",
    "        \n",
    "        (f\"What is the memory capacity of {api_name}?\", \n",
    "         f\"The memory capacity of {api_name} is {memory}. This {name} instance is classified as {family} and offers {vcpus} at a rate of {price}.\"),\n",
    "        \n",
    "        (f\"How much does it cost to run {api_name} for one hour?\", \n",
    "         f\"Running {api_name} for one hour costs {price}. The {name} instance provides {memory} and {vcpus}, optimized for {family} use cases.\"),\n",
    "        \n",
    "        (f\"What is the vCPU count for {name}?\", \n",
    "         f\"The vCPU count for {name} is {vcpus}. This instance ({api_name}) is a {family} type with {memory} of memory, costing {price} per hour.\"),\n",
    "        \n",
    "        (f\"Which EC2 instance is called {name}?\", \n",
    "         f\"The EC2 instance called {name} has the API name {api_name}. It's a {family} instance featuring {memory}, {vcpus}, and priced at {price}.\"),\n",
    "        \n",
    "        (f\"What type of workload is {api_name} optimized for?\", \n",
    "         f\"The {api_name} is optimized for {family} workloads. This {name} instance comes with {memory}, {vcpus}, and costs {price} on-demand.\"),\n",
    "        \n",
    "        (f\"Describe the {api_name} instance configuration.\", \n",
    "         f\"The {api_name} instance configuration includes: Name - {name}, Family - {family}, Memory - {memory}, vCPUs - {vcpus}, and hourly cost of {price}.\"),\n",
    "        \n",
    "        (f\"What are the key features of {api_name}?\", \n",
    "         f\"Key features of {api_name} ({name}): It's a {family} instance with {memory} RAM, {vcpus} processing power, available at {price} per hour on-demand.\"),\n",
    "    ]\n",
    "    \n",
    "    for q, a in questions:\n",
    "        qa_pairs.append({'question': q, 'answer': a})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "938578d5-41af-4c07-88f4-0cf616b5c1c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25347"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5ef9b0f3-19d4-40f0-8939-6540490a6713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 25347 Q&A pairs and saved to ec2_pricing_dataset.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 25347 examples [00:00, 683188.57 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 25347\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('ec2_pricing_dataset.jsonl', 'w') as f:\n",
    "    for pair in qa_pairs:\n",
    "        f.write(json.dumps(pair) + '\\n')\n",
    "\n",
    "print(f\"Generated {len(qa_pairs)} Q&A pairs and saved to ec2_pricing_dataset.jsonl\")\n",
    "\n",
    "# Now load with datasets\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"ec2_pricing_dataset.jsonl\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2ff92b61-71cc-496e-9322-3f4583e07a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1fd2b671-50bf-4292-970f-5202801afbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"ec2_pricing_dataset.jsonl\",split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "54d31383-c5fe-4584-9d22-66d4bd4fafa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_n = dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "845deade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ff5e7307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "# Force CPU usage\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "torch.set_default_device(\"cpu\")\n",
    "\n",
    "# Clear any MPS cache\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "aa004e29-a569-4529-a438-207da80a6098",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/t5-efficient-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047ef341",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e9543399-4214-416e-8d90-0a0e916dcb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 22812/22812 [00:02<00:00, 8979.40 examples/s] \n",
      "Map: 100%|██████████| 2535/2535 [00:00<00:00, 3359.28 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# def preprocess_function(examples):\n",
    "#     # Combine question and answer in instruction format\n",
    "#     prompts = [f\"Question: {q}\\nAnswer: {a}\" for q, a in zip(examples[\"question\"], examples[\"answer\"])]\n",
    "    \n",
    "#     return tokenizer(\n",
    "#         prompts,\n",
    "#         truncation=True,\n",
    "#         max_length=512,\n",
    "#         padding=\"max_length\"\n",
    "#     )\n",
    "# tokenized_dataset = dataset_n.map(preprocess_function, batched=True)\n",
    "def preprocess_function(examples):\n",
    "    # tokenize both the questions (input) and answers (target)\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        text_target=examples[\"answer\"],  # pass answers as the target text\n",
    "        max_length=256,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset_n.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e81e9dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886bf4de-a197-42b0-b87b-6a5ddaebf633",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664c9af3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5995e629-1fc8-4425-931a-afc948b0c3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train = tokenized_dataset[\"train\"].shuffle(seed=42).select(range(5000))\n",
    "small_eval = tokenized_dataset[\"test\"].shuffle(seed=42).select(range(1000))\n",
    "# small_train = tokenized_dataset[\"train\"]\n",
    "# small_eval = tokenized_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e9f30b71-1000-4842-9512-543f36849ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # extract logits from the predictions tuple\n",
    "    predictions = predictions[0]\n",
    "\n",
    "    # ensure predictions are numpy arrays before processing\n",
    "    # also, clamp to ensure token IDs are within valid range if model outputs unexpected values\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    # replace -100 in labels with the tokenizer's pad_token_id for correct decoding\n",
    "    # this also ensures labels are a numpy array of integers\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # format labels as a list of lists for rouge metric, which expects multiple references\n",
    "    formatted_labels = [[label] for label in decoded_labels]\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=formatted_labels)\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0ea275",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c251bbaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1e244e85-7005-4ab1-9e81-5a6adc5ca6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"ec2_price_prediction\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fb407e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6723018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "252b784a-910e-40b1-bc52-dfa239cb79ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 92/92 [00:00<00:00, 1928.94it/s, Materializing param=shared.weight]                                                       \n",
      "The tied weights mapping and config for this model specifies to tie shared.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
      "The tied weights mapping and config for this model specifies to tie shared.weight to encoder.embed_tokens.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
      "The tied weights mapping and config for this model specifies to tie shared.weight to decoder.embed_tokens.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoModelForCausalLM,AutoModelForSeq2SeqLM\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/t5-efficient-mini\", num_labels=5)\n",
    "model = model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7dee9bf0-c18f-4f69-84d4-ddd1fe8e5160",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a8828b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch_cuda_alloc_conf is set to: expandable_segments:true\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# this line helps pytorch manage gpu memory more efficiently\n",
    "# by allowing memory segments to expand, reducing fragmentation.\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:true\"\n",
    "\n",
    "# now you can import torch and proceed with your model loading and training\n",
    "import torch\n",
    "\n",
    "# you might also want to confirm it's set\n",
    "print(f\"pytorch_cuda_alloc_conf is set to: {os.environ.get('PYTORCH_CUDA_ALLOC_CONF')}\")\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ff336c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a7074c8b-6563-4f63-b618-e1d013f1bdaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shashidherreddymaram/ec2_pricing_prediction_finetunning/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='562' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 562/1875 13:22 < 31:20, 0.70 it/s, Epoch 0.90/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m      4\u001b[39m training_args = TrainingArguments(\n\u001b[32m      5\u001b[39m     output_dir=output_dir_model,\n\u001b[32m      6\u001b[39m     eval_strategy=\u001b[33m\"\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m     \u001b[38;5;66;03m# gradient_checkpointing=True,\u001b[39;00m\n\u001b[32m     22\u001b[39m )\n\u001b[32m     25\u001b[39m trainer = Trainer(\n\u001b[32m     26\u001b[39m     model=model,\n\u001b[32m     27\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     31\u001b[39m     compute_metrics=compute_metrics,\n\u001b[32m     32\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ec2_pricing_prediction_finetunning/.venv/lib/python3.13/site-packages/transformers/trainer.py:2170\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2168\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2169\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2170\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2171\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2172\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2173\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2174\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2175\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ec2_pricing_prediction_finetunning/.venv/lib/python3.13/site-packages/transformers/trainer.py:2592\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2589\u001b[39m     context = implicit_replication\n\u001b[32m   2591\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2592\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2594\u001b[39m \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_optimizer_step(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m   2596\u001b[39m \u001b[38;5;66;03m# get leaning rate before update\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ec2_pricing_prediction_finetunning/.venv/lib/python3.13/site-packages/torch/optim/lr_scheduler.py:166\u001b[39m, in \u001b[36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    164\u001b[39m opt = opt_ref()\n\u001b[32m    165\u001b[39m opt._opt_called = \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ec2_pricing_prediction_finetunning/.venv/lib/python3.13/site-packages/accelerate/optimizer.py:179\u001b[39m, in \u001b[36mAcceleratedOptimizer.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    177\u001b[39m         \u001b[38;5;28mself\u001b[39m._accelerate_step_called = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator_state.distributed_type == DistributedType.XLA:\n\u001b[32m    181\u001b[39m     \u001b[38;5;28mself\u001b[39m.gradient_state.is_xla_gradients_synced = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ec2_pricing_prediction_finetunning/.venv/lib/python3.13/site-packages/torch/optim/optimizer.py:526\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    521\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    522\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    523\u001b[39m             )\n\u001b[32m    525\u001b[39m \u001b[38;5;66;03m# pyrefly: ignore [invalid-param-spec]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m526\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    529\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ec2_pricing_prediction_finetunning/.venv/lib/python3.13/site-packages/torch/optim/optimizer.py:81\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     79\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     80\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     83\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ec2_pricing_prediction_finetunning/.venv/lib/python3.13/site-packages/torch/optim/adam.py:248\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    236\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    238\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    239\u001b[39m         group,\n\u001b[32m    240\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    245\u001b[39m         state_steps,\n\u001b[32m    246\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ec2_pricing_prediction_finetunning/.venv/lib/python3.13/site-packages/torch/optim/optimizer.py:151\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    149\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ec2_pricing_prediction_finetunning/.venv/lib/python3.13/site-packages/torch/optim/adam.py:971\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    969\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m971\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    988\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    989\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    990\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    991\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ec2_pricing_prediction_finetunning/.venv/lib/python3.13/site-packages/torch/optim/adam.py:880\u001b[39m, in \u001b[36m_fused_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    878\u001b[39m func = torch._fused_adam_ \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m decoupled_weight_decay \u001b[38;5;28;01melse\u001b[39;00m torch._fused_adamw_\n\u001b[32m    879\u001b[39m \u001b[38;5;66;03m# pyrefly: ignore [no-matching-overload]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m880\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_grads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_exp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_max_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m    886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_state_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m    889\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    891\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    892\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    893\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    894\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_grad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_found_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    896\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    897\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m device_found_inf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    898\u001b[39m     torch._foreach_sub_(\n\u001b[32m    899\u001b[39m         device_state_steps, [device_found_inf] * \u001b[38;5;28mlen\u001b[39m(device_state_steps)\n\u001b[32m    900\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ec2_pricing_prediction_finetunning/.venv/lib/python3.13/site-packages/torch/utils/_device.py:116\u001b[39m, in \u001b[36mDeviceContext.__torch_function__\u001b[39m\u001b[34m(self, func, types, args, kwargs)\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    115\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.device\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from transformers import Trainer,TrainingArguments\n",
    "output_dir_model = \"./ec2_qa_model\"\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir_model,\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    # gradient_accumulation_steps=2,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    "    fp16=False,  # MPS doesn't support fp16, use default\n",
    "    dataloader_num_workers=0,\n",
    "    # no_cuda = True,\n",
    "    logging_steps=100,\n",
    "    save_total_limit=1,\n",
    "    # gradient_checkpointing=True,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train,\n",
    "    eval_dataset=small_eval,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc79bf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./ec2_qa_final_model\")\n",
    "tokenizer.save_pretrained(\"./ec2_qa_final_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4703da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 92/92 [00:00<00:00, 1696.79it/s, Materializing param=shared.weight]                                                       \n",
      "The tied weights mapping and config for this model specifies to tie shared.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
      "The tied weights mapping and config for this model specifies to tie shared.weight to encoder.embed_tokens.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
      "The tied weights mapping and config for this model specifies to tie shared.weight to decoder.embed_tokens.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 384)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 384)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=384, out_features=1536, bias=False)\n",
       "              (wo): Linear(in_features=1536, out_features=384, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-3): 3 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=384, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=384, out_features=1536, bias=False)\n",
       "              (wo): Linear(in_features=1536, out_features=384, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 384)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=384, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=384, out_features=1536, bias=False)\n",
       "              (wo): Linear(in_features=1536, out_features=384, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-3): 3 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=384, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=384, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=384, out_features=1536, bias=False)\n",
       "              (wo): Linear(in_features=1536, out_features=384, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=384, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model = AutoModelForSeq2SeqLM.from_pretrained(\"ec2_qa_model/checkpoint-15\")\n",
    "my_tokenizer = AutoTokenizer.from_pretrained(\"ec2_qa_model/checkpoint-15\")\n",
    "model = model.to(\"cpu\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26eecae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_length=100):\n",
    "    inputs = my_tokenizer(prompt, return_tensors=\"pt\").to(\"cpu\")\n",
    "    \n",
    "    outputs = my_model.generate(\n",
    "        **inputs,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Test with a sample prompt\n",
    "prompt = \"EC2 instance\"\n",
    "generated = generate_text(prompt)\n",
    "print(generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
